{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Joshua Goldshteyn\n",
    "Graph Machine Learning Final Project SimpliText\n",
    "2/21/2023\n",
    "Main Runner Notebook"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Joshua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Main Program to generate simplifications of sentences\n",
    "# importing all necessary modules\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import warnings\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "from pattern.text.en import singularize, pluralize\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load Word2Vec and Doc2Vec models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "w2v_english = pickle.load(open('./w2v_english.pkl', 'rb'))\n",
    "d2v_english = pickle.load(open('./d2v_english.pkl', 'rb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate a Simple English Vocabulary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "vocab_set = set()\n",
    "with open('simple_wikipedia_filtered_abstracts.xml', 'r') as file:\n",
    "    data = file.read()\n",
    "    tokenized = wordpunct_tokenize(data)\n",
    "    fdist = FreqDist(tokenized)\n",
    "    full_set = list(filter(lambda x: x[1]>=10, fdist.items()))\n",
    "    vocab_set = set(filter(lambda x:x.isalpha(), map(lambda x:x.lower(),map(lambda x: x[0], full_set))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15631\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab_set))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#Adapted from https://drumcoder.co.uk/blog/2013/dec/23/finding-proper-nouns-nltk/\n",
    "def findtags(tag_prefix, tagged_text):\n",
    "    \"\"\"\n",
    "    Find tokens matching the specified tag_prefix\n",
    "    \"\"\"\n",
    "    cfd = nltk.ConditionalFreqDist((tag, word) for (word, tag) in tagged_text\n",
    "                                  if tag.startswith(tag_prefix))\n",
    "    return dict((tag, cfd[tag].keys()) for tag in cfd.conditions())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def replace_word(index, sentence):\n",
    "    word = sentence[index]\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for i in syn.lemmas():\n",
    "            synonyms.add(i.name())\n",
    "            #if i.name() not in vocab_set:\n",
    "\n",
    "    synonyms = list(synonyms)\n",
    "    #print(word, \"SYNONYMS ARE\", synonyms)\n",
    "    #print(set(synonyms))\n",
    "    #print(set(synonyms) & vocab_set)\n",
    "    possible_synonyms = set(synonyms).intersection(vocab_set)\n",
    "    #print(possible_synonyms)\n",
    "    if len(possible_synonyms)==0:\n",
    "        return \"|\"+word+\"|\"\n",
    "    #print(word, \"POSSIBLE SYNONYMS ARE\", possible_synonyms)\n",
    "\n",
    "    word_affinities = {}\n",
    "    for cur_word in possible_synonyms:\n",
    "        if cur_word in w2v_english.wv.key_to_index and word in w2v_english.wv.key_to_index:\n",
    "            w2v_score = w2v_english.wv.similarity(cur_word, word)\n",
    "        else:\n",
    "            w2v_score = 0.25\n",
    "        #Doc2Vec is very numerically unstable in this use case, and needs to be averaged to get a good value\n",
    "        sample_count = 25\n",
    "        d2v_score = 0\n",
    "        test_sentence = sentence.copy()\n",
    "        for x in range(sample_count):\n",
    "            test_sentence[index] = cur_word\n",
    "            orig = d2v_english.infer_vector(sentence)\n",
    "            new = d2v_english.infer_vector(test_sentence)\n",
    "            d2v_score += np.dot(orig,new)/(norm(orig)*norm(new))\n",
    "        d2v_score /= sample_count\n",
    "        word_affinities[cur_word] = (w2v_score + d2v_score)/2\n",
    "    #print(\"Word Affinities are: \")\n",
    "\n",
    "    ordered_affinities = sorted(word_affinities.items(), key=lambda x:x[1], reverse=True)\n",
    "    #print(ordered_affinities)\n",
    "    chosen_word = ordered_affinities[0][0]\n",
    "    is_singular = word == singularize(word)\n",
    "    if is_singular:\n",
    "        chosen_word = singularize(chosen_word)\n",
    "    else:\n",
    "        chosen_word = pluralize(chosen_word)\n",
    "    if word == chosen_word:\n",
    "        return chosen_word\n",
    "    else:\n",
    "        return \"{\"+word+\"-->\"+chosen_word+\"}\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def run_simplify(text_to_simplify):\n",
    "    sentences = sent_tokenize(text_to_simplify)\n",
    "    output = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        current_list = word_tokenize(sentence)\n",
    "        proper_nouns = set()\n",
    "        proper_nouns_tagged = findtags('NNP', nltk.pos_tag(current_list)).get('NNP')\n",
    "\n",
    "        if proper_nouns_tagged != None:\n",
    "            proper_nouns = set(proper_nouns_tagged)\n",
    "        #print(proper_nouns)\n",
    "        for x in range(len(current_list)):\n",
    "            if current_list[x].isalpha() and current_list[x].lower() not in vocab_set and current_list[x] not in proper_nouns:\n",
    "\n",
    "                current_list[x] = replace_word(x, current_list)\n",
    "        output+=\" \".join(current_list)\n",
    "        #print(current_list)\n",
    "    #print(sentences)\n",
    "    output = output.replace(\" , \", \", \")\n",
    "    output = output.replace(\" .\", \". \")\n",
    "    output = output.replace(\" !\", \"! \")\n",
    "    output = output.replace(\" ?\", \".? \")\n",
    "    output = output.replace(\" ; \", \"; \")\n",
    "    output = output.replace(\" : \", \": \")\n",
    "    output = output.replace(\" 's\", \"'s\")\n",
    "    output = output.replace(\" ( \", \" (\")\n",
    "    output = output.replace(\" ) \", \") \")\n",
    "    output = output.replace(\" );\", \");\")\n",
    "    output = output.replace(\" ).\", \").\")\n",
    "    print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Graph neural network (GNN) is a class of artificial neural networks for processing data that can be represented as graphs. In the more general subject of `` Geometric Deep Learning '', certain existing neural network architectures can be {interpreted-->interpret} as GNNs operating on |suitably| defined graphs. [ 4 ] Convolutional neural networks, in the context of computer vision, can be seen as a GNN applied to graphs structured as grids of pixels. Transformers, in the context of natural language processing, can be seen as GNNs applied to complete graphs whose nodes are words in a sentence. The key design element of GNNs is the use of |pairwise| message passing, such that graph nodes |iteratively| |update| their representations by {exchanging-->exchange} information with their |neighbors|. Since their {inception-->origin}, several different GNN architectures have been proposed, [ 1 ] [ 5 ] [ 6 ] [ 7 ] which {implement-->enforce} different flavors of message passing. [ 4 ] As of 2022, whether it is possible to define GNN architectures `` going beyond '' message passing, or if every GNN can be built on message passing over |suitably| defined graphs, is an open research question. [ 8 ] Relevant application domains for GNNs include social networks, [ 9 ] {citation-->mention} networks, [ 10 ] molecular biology, [ 11 ] chemistry, [ 12 ] physics [ 13 ] and NP-hard |combinatorial| optimization problems. [ 14 ] Several open source libraries {implementing-->enforce} graph neural networks are available, such as PyTorch Geometric [ 15 ] (PyTorch ), TensorFlow GNN [ 16 ] (TensorFlow ), and |jraph| [ 17 ] (Google JAX). The architecture of a generic GNN {implements-->applies} the following fundamental layers: [ 4 ] Permutation |equivariant|: a {permutation-->substitution} |equivariant| layer maps a representation of a graph into an updated representation of the same graph. In the literature, {permutation-->substitution} |equivariant| layers are {implemented-->apply} via |pairwise| message passing between graph nodes. [ 4 ] [ 8 ] Intuitively, in a message passing layer, nodes |update| their representations by {aggregating-->combine} the messages received from their immediate neighbours. As such, each message passing layer increases the {receptive-->sensory} field of the GNN by one hop. Local {pooling-->pool}: a local {pooling-->pool} layer |coarsens| the graph via |downsampling|. Local {pooling-->pool} is used to increase the {receptive-->sensory} field of a GNN, in a similar fashion to {pooling-->pool} layers in |convolutional| neural networks. Examples include k-nearest neighbours {pooling-->pool}, top-k {pooling-->pool}, [ 18 ] and self-attention {pooling-->pool}. [ 19 ] Global {pooling-->pool}: a global {pooling-->pool} layer, also known as |readout| layer, provides fixed-size representation of the whole graph. The global {pooling-->pool} layer must be {permutation-->substitution} {invariant-->constant}, such that {permutations-->substitutions} in the ordering of graph nodes and edges do not {alter-->change} the final output. [ 20 ] Examples include element-wise sum, mean or maximum. It has been {demonstrated-->prove} that GNNs can not be more |expressive| than the Weisfeiler–Lehman Graph Isomorphism Test. [ 21 ] [ 22 ] In practice, this means that there exist different graph structures (e.g., molecules with the same atoms but different bonds) that can not be distinguished by GNNs. More powerful GNNs operating on higher-dimension geometries such as |simplicial| complexes can be designed. [ 23 ] As of 2022, whether or not future architectures will {overcome-->defeat} the message passing primitive is an open research question. [ 8 ]\n"
     ]
    }
   ],
   "source": [
    "GNN_text =\"\"\"A Graph neural network (GNN) is a class of artificial neural networks for processing data that can be represented as graphs.\n",
    "In the more general subject of \"Geometric Deep Learning\", certain existing neural network architectures can be interpreted as GNNs operating on suitably defined graphs.[4] Convolutional neural networks, in the context of computer vision, can be seen as a GNN applied to graphs structured as grids of pixels. Transformers, in the context of natural language processing, can be seen as GNNs applied to complete graphs whose nodes are words in a sentence.\n",
    "\n",
    "The key design element of GNNs is the use of pairwise message passing, such that graph nodes iteratively update their representations by exchanging information with their neighbors. Since their inception, several different GNN architectures have been proposed,[1][5][6][7] which implement different flavors of message passing.[4] As of 2022, whether it is possible to define GNN architectures \"going beyond\" message passing, or if every GNN can be built on message passing over suitably defined graphs, is an open research question.[8]\n",
    "\n",
    "Relevant application domains for GNNs include social networks,[9] citation networks,[10] molecular biology,[11] chemistry,[12] physics[13] and NP-hard combinatorial optimization problems.[14]\n",
    "\n",
    "Several open source libraries implementing graph neural networks are available, such as PyTorch Geometric[15] (PyTorch), TensorFlow GNN[16] (TensorFlow), and jraph[17] (Google JAX).\n",
    "\n",
    "The architecture of a generic GNN implements the following fundamental layers:[4]\n",
    "\n",
    "Permutation equivariant: a permutation equivariant layer maps a representation of a graph into an updated representation of the same graph. In the literature, permutation equivariant layers are implemented via pairwise message passing between graph nodes.[4][8] Intuitively, in a message passing layer, nodes update their representations by aggregating the messages received from their immediate neighbours. As such, each message passing layer increases the receptive field of the GNN by one hop.\n",
    "Local pooling: a local pooling layer coarsens the graph via downsampling. Local pooling is used to increase the receptive field of a GNN, in a similar fashion to pooling layers in convolutional neural networks. Examples include k-nearest neighbours pooling, top-k pooling,[18] and self-attention pooling.[19]\n",
    "Global pooling: a global pooling layer, also known as readout layer, provides fixed-size representation of the whole graph. The global pooling layer must be permutation invariant, such that permutations in the ordering of graph nodes and edges do not alter the final output.[20] Examples include element-wise sum, mean or maximum.\n",
    "It has been demonstrated that GNNs cannot be more expressive than the Weisfeiler–Lehman Graph Isomorphism Test.[21][22] In practice, this means that there exist different graph structures (e.g., molecules with the same atoms but different bonds) that cannot be distinguished by GNNs. More powerful GNNs operating on higher-dimension geometries such as simplicial complexes can be designed.[23] As of 2022, whether or not future architectures will overcome the message passing primitive is an open research question.[8]\n",
    "\"\"\"\n",
    "\n",
    "run_simplify(GNN_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "ML_text = \"\"\"Machine learning (ML) is a field of inquiry devoted to understanding and building methods that \"learn\" – that is, methods that leverage data to improve performance on some set of tasks.[1] It is seen as a part of artificial intelligence.\n",
    "\n",
    "Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so.[2] Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, agriculture, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.[3][4]\n",
    "\n",
    "A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers, but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.[6][7]\n",
    "\n",
    "Some implementations of machine learning use data and neural networks in a way that mimics the working of a biological brain.[8][9]\n",
    "\n",
    "In its application across business problems, machine learning is also referred to as predictive analytics.\n",
    "Learning algorithms work on the basis that strategies, algorithms, and inferences that worked well in the past are likely to continue working well in the future. These inferences can be obvious, such as \"since the sun rose every morning for the last 10,000 days, it will probably rise tomorrow morning as well\". They can be nuanced, such as \"X% of families have geographically separate species with color variants, so there is a Y% chance that undiscovered black swans exist\".[10]\n",
    "\n",
    "Machine learning programs can perform tasks without being explicitly programmed to do so. It involves computers learning from data provided so that they carry out certain tasks. For simple tasks assigned to computers, it is possible to program algorithms telling the machine how to execute all steps required to solve the problem at hand; on the computer's part, no learning is needed. For more advanced tasks, it can be challenging for a human to manually create the needed algorithms. In practice, it can turn out to be more effective to help the machine develop its own algorithm, rather than having human programmers specify every needed step.[11]\n",
    "\n",
    "The discipline of machine learning employs various approaches to teach computers to accomplish tasks where no fully satisfactory algorithm is available. In cases where vast numbers of potential answers exist, one approach is to label some of the correct answers as valid. This can then be used as training data for the computer to improve the algorithm(s) it uses to determine correct answers. For example, to train a system for the task of digital character recognition, the MNIST dataset of handwritten digits has often been used.\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning (ML) is a field of {inquiry-->research} devoted to understanding and building methods that `` learn '' – that is, methods that {leverage-->purchase} data to improve performance on some set of tasks. [ 1 ] It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being |explicitly| programmed to do so. [ 2 ] Machine learning algorithms are used in a wide variety of applications, such as in medicine, email {filtering-->filter}, speech recognition, agriculture, and computer vision, where it is difficult or |unfeasible| to develop conventional algorithms to perform the needed tasks. [ 3 ] [ 4 ] A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers, but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on |exploratory| data analysis through |unsupervised| learning. [ 6 ] [ 7 ] Some {implementations-->executions} of machine learning use data and neural networks in a way that |mimics| the working of a biological brain. [ 8 ] [ 9 ] In its application across business problems, machine learning is also referred to as |predictive| |analytics|. Learning algorithms work on the basis that strategies, algorithms, and inferences that worked well in the past are likely to continue working well in the future. These inferences can be |obvious|, such as `` since the sun rose every morning for the last 10,000 days, it will probably rise tomorrow morning as well ''. They can be |nuanced|, such as `` X % of families have geographically separate species with color variants, so there is a Y % chance that |undiscovered| black swans exist ''. [ 10 ] Machine learning programs can perform tasks without being |explicitly| programmed to do so. It involves computers learning from data provided so that they carry out certain tasks. For simple tasks assigned to computers, it is possible to program algorithms telling the machine how to execute all steps required to solve the problem at hand; on the computer's part, no learning is needed. For more advanced tasks, it can be {challenging-->dispute} for a human to |manually| create the needed algorithms. In practice, it can turn out to be more effective to help the machine develop its own algorithm, rather than having human programmers specify every needed step. [ 11 ] The discipline of machine learning {employs-->engages} various approaches to teach computers to {accomplish-->achieve} tasks where no fully {satisfactory-->acceptable} algorithm is available. In cases where vast numbers of potential answers exist, one approach is to label some of the correct answers as valid. This can then be used as training data for the computer to improve the algorithm (s) it uses to determine correct answers. For example, to train a system for the task of digital character recognition, the MNIST |dataset| of |handwritten| digits has often been used. \n"
     ]
    }
   ],
   "source": [
    "run_simplify(ML_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "pagerank_text = \"\"\"\n",
    "The importance of a Web page is an inherently sub jective matter, which depends on the\n",
    "readers interests, knowledge and attitudes. But there is still much that can be said ob jectively\n",
    "about the relative importance of Web pages. This paper describes PageRank, a method for\n",
    "rating Web pages ob jectively and mechanically, effectively measuring the human interest and\n",
    "attention devoted to them.\n",
    "We compare PageRank to an idealized random Web surfer. We show how to e\u000Eciently\n",
    "compute PageRank for large numbers of pages. And, we show how to apply PageRank to search\n",
    "and to user navigation.\n",
    "1 Introduction and Motivation\n",
    "The World Wide Web creates many new challenges for information retrieval. It is very large and\n",
    "heterogeneous. Current estimates are that there are over 150 million web pages with a doubling\n",
    "life of less than one year. More importantly, the web pages are extremely diverse, ranging from\n",
    "\"What is Joe having for lunch today?\" to journals about information retrieval. In addition to these\n",
    "ma jor challenges, search engines on the Web must also contend with inexperienced users and pages\n",
    "engineered to manipulate search engine ranking functions.\n",
    "However, unlike \"\n",
    "at\" document collections, the World Wide Web is hypertext and provides\n",
    "considerable auxiliary information on top of the text of the web pages, such as link structure and\n",
    "link text. In this paper, we take advantage of the link structure of the Web to produce a global\n",
    "\\importance\" ranking of every web page. This ranking, called PageRank, helps search engines and\n",
    "users quickly make sense of the vast heterogeneity of the World Wide Web.\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The importance of a Web page is an |inherently| sub |jective| matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said |ob| |jectively| about the relative importance of Web pages. This paper describes PageRank, a method for rating Web pages |ob| |jectively| and {mechanically-->automatically}, |effectively| measuring the human interest and attention devoted to them. We compare PageRank to an |idealized| random Web surfer. We show how to e\u000Eciently {compute-->calculate} PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation. 1 Introduction and Motivation The World Wide Web creates many new challenges for information |retrieval|. It is very large and |heterogeneous|. Current estimates are that there are over 150 million web pages with a {doubling-->double} life of less than one year. More {importantly-->significantly}, the web pages are extremely diverse, ranging from '' What is Joe having for |lunch| today.?  ''to journals about information |retrieval|. In addition to these ma |jor| challenges, search engines on the Web must also {contend-->contest} with |inexperienced| users and pages {engineered-->organize} to {manipulate-->fake} search engine ranking functions. However, unlike `` at '' document collections, the World Wide Web is |hypertext| and provides considerable auxiliary information on top of the text of the web pages, such as link structure and link text. In this paper, we take advantage of the link structure of the Web to produce a global \\importance '' ranking of every web page. This ranking, called PageRank, helps search engines and users quickly make sense of the vast |heterogeneity| of the World Wide Web. \n"
     ]
    }
   ],
   "source": [
    "run_simplify(pagerank_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}