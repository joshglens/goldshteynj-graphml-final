Graph Machine Learning Final Project: Simplitext  
Joshua Goldshteyn  
2/21/2023  

# Abstract

In many cases, Wikipedia and other technical oriented sources may use complex words and other jargon when simpler words would work much better. This is due to the fact that the writers of those articles want to demonstrate their level of knowledge and experience in the field, as well as falling into the “Curse of Knowledge”, where many of the written articles assume that a reader has an adequate level of background knowledge. As a result, there are articles that are unnecessarily complex. This project is both interesting and important due to the importance that equitable, easy access to knowledge has in order to help both lifelong learners stay up to date, as well as educate the public on various techniques that may be used in fields to increase their general level of knowledge. The hypothesis being presented is that a combination of graph based NLP models, specifically both Word2Vec and Doc2Vec, when combined, will have the ability to create noticeable decreases in the complexity of provided writing without losing a significant amount of meaning. My contribution is the combination of both models, and using a synonym model provided by NLTK to generate potential options to reduce the complexity of speech while running on a Wikipedia abstract dataset. The results will be measured by running various Wikipedia articles through it, and seeing how the replacement performs. While this project did not perform very well, there were some areas where a word was replaced and simplified which would have helped a non-native English speaker and the general public with understanding.

# Introduction

There are many methods to get meaning out of words, whether that is through a bag-of-words approach or through embeddings. The methods chosen in this project all rely on embedding vectors of specific sizes to encapsulate the meaning of various words and sentences. In this specific case, the purpose is to simplify complex words, which would mean that the replacement words would be more commonly found while still keeping the meaning of the words they are replacing and the sentences they are used in intact.

There are two models which are used to calculate embedding vectors given synonyms. These synonyms are generated using NLTK’s synset, which generates all possible synonyms for a word in all of its situations. The two models are used to compute cosine similarity scores between a word and its replacement, or two sentences with the word replaced to find the best words. The similarity scores are then averaged to generate a context-aware replacement similarity score. These two models are now described below:

The first model used is known as Word2Vec. This model is used to create embeddings by trying “to maximize classification of a word based on another word in the same sentence” [1]. This is also caused due to the skip-gram architecture, which allows words to use other words that are not exactly adjacent but with word “skips” to further improve the meaning captured by the embedding vectors. This paper also described a continuous bag of words architecture that is used to predict words given context, whereas the skip-gram model predicts the context given a word. In the paper’s results section, the skip-gram model has a much higher semantic accuracy than the continuous bag of words model, which is more important in the application of this project where the semantic meaning must be preserved when looking at nearby words in the embedding space.

The second model used is known as Doc2Vec. The purpose of this model is to get semantic meaning from various lengths of input text or input vectors, in a fixed length output embedding vector. These are created using learned paragraph vectors that are similar to averaging the meaning of the raw word vectors; “the only change is the additional paragraph token that is mapped to a vector via matrix D” [2]. The version used in this project is known as the “Distributed Memory Model of Paragraph Vectors (PV-DM)”, which is generally used for learning vectors for next word prediction. As a result, this type of model is promising for creating word predictions even given surrounding context rather than trying to predict the next word in a series.

# Methods

The dataset used for this project is entirely created from abstract uploads available using XML format Wikimedia dumps. The two datasets used are the English Wikipedia Abstracts dataset (enwiki-latest-abstract.xml.gz, size of 845 MB compressed), and the Simple English Wikipedia Abstracts dataset (simplewiki-latest-abstract.xml.gz, size of 202 MB compressed). Both of these datasets were pre-processed to strip away additional xml headings that would get in the way of embedding, especially since they could potentially “cloud” the embedding space as those tags appear near almost all types of content.

After preprocessing, the first model that was trained was the Word2Vec using the English abstracts. To do this, the abstracts are first loaded into the notebook and split into a list of lists, where each sublist is a sentence in the larger list that contains all sentences, and words, which are made lowercase, are the individual tokens. Then, these tokens are fed into a skip gram model, with a window of 5 words, a minimum document frequency of 10 words, and an embedding vector size of 100. Going above 100 did not yield significantly better results, while going below 100 seemed to yield additional instability between runs, so the embedding size of 100 was chosen.

After this, a Doc2Vec model was trained, also using the English abstracts. However, instead of being manually preprocessed into lists of lists, these are instead converted into lists of “Tagged Documents” (with all words made undercase), which are then fed into the model. Every document in this Doc2Vec model is a different abstract for a specific article. As a result, this model instead had an embedding vector of 200, and did not use the skip gram techniques which allowed for faster training, although training for only 60 epochs took approximately 3 hours on an i7-11700k, as there was no GPU implementation for this model. The size of 200 for the embedding vector was chosen because the concepts described by a sentence can likely have a more complicated concept than a single word, however not much more so due to the fact that the abstracts presented in the dataset all correspond to a description of an article about one or two word subjects.

After Word2Vec and Doc2Vec models were trained, they were loaded into a final testing notebook. The Simple English Wikipedia abstracts were used to create a vocabulary bank of potential replacement words. In this notebook, a paragraph to be simplified was also input. Each paragraph was split into a list of sentences. Each sentence was iterated across, and if a word that became lowercase did not appear in the Simple English vocabulary bank, it was then sent, with the rest of the sentence for context, to be replaced. This replacement is done by using NLTK to generate a list of potential synonyms for the word to be replaced, and then checking those synonyms against the vocabulary bank. If they are in the bank, every possible synonym is then compared against the original word using Word2Vec for its similarity score, as well as replaced in the sentence and compared with the original sentence with Doc2Vec for a contextual similarity score. These two scores are then averaged to get the “best” replacement, and allow for different synonyms to replace the same word depending on the context. Some minor post processing is then done on the resulting text to mitigate improper spacing that occurs as a result of the reconstruction. 

# Results

The results are read as follows. For words such as |this|, the word did not appear in the Simple English vocabulary bank, but neither did any of its synonyms, so it is left as is. Words that were successfully replaced have the style of {from–>to}, to show the original word and the word that the models decided was best.


This first example is running the model on the overview and first section of GNN’s on Wikipedia:

A Graph neural network (GNN) is a class of artificial neural networks for processing data that can be represented as graphs. In the more general subject of '' Geometric Deep Learning '', certain existing neural network architectures can be {interpreted-->interpret} as GNNs operating on |suitably| defined graphs. [ 4 ] Convolutional neural networks, in the context of computer vision, can be seen as a GNN applied to graphs structured as grids of pixels. Transformers, in the context of natural language processing, can be seen as GNNs applied to complete graphs whose nodes are words in a sentence. The key design element of GNNs is the use of |pairwise| message passing, such that graph nodes |iteratively| |update| their representations by {exchanging-->exchange} information with their |neighbors|. Since their {inception-->origin}, several different GNN architectures have been proposed, [ 1 ] [ 5 ] [ 6 ] [ 7 ] which {implement-->enforce} different flavors of message passing. [ 4 ] As of 2022, whether it is possible to define GNN architectures `` going beyond '' message passing, or if every GNN can be built on message passing over |suitably| defined graphs, is an open research question. [ 8 ] Relevant application domains for GNNs include social networks, [ 9 ] {citation-->mention} networks, [ 10 ] molecular biology, [ 11 ] chemistry, [ 12 ] physics [ 13 ] and NP-hard |combinatorial| optimization problems. [ 14 ] Several open source libraries {implementing-->enforce} graph neural networks are available, such as PyTorch Geometric [ 15 ] (PyTorch ), TensorFlow GNN [ 16 ] (TensorFlow ), and |jraph| [ 17 ] (Google JAX). The architecture of a generic GNN {implements-->applies} the following fundamental layers: [ 4 ] Permutation |equivariant|: a {permutation-->substitution} |equivariant| layer maps a representation of a graph into an updated representation of the same graph. In the literature, {permutation-->substitution} |equivariant| layers are {implemented-->apply} via |pairwise| message passing between graph nodes. [ 4 ] [ 8 ] Intuitively, in a message passing layer, nodes |update| their representations by {aggregating-->combine} the messages received from their immediate neighbours. As such, each message passing layer increases the {receptive-->sensory} field of the GNN by one hop. Local {pooling-->pool}: a local {pooling-->pool} layer |coarsens| the graph via |downsampling|. Local {pooling-->pool} is used to increase the {receptive-->sensory} field of a GNN, in a similar fashion to {pooling-->pool} layers in |convolutional| neural networks. Examples include k-nearest neighbours {pooling-->pool}, top-k {pooling-->pool}, [ 18 ] and self-attention {pooling-->pool}. [ 19 ] Global {pooling-->pool}: a global {pooling-->pool} layer, also known as |readout| layer, provides fixed-size representation of the whole graph. The global {pooling-->pool} layer must be {permutation-->substitution} {invariant-->constant}, such that {permutations-->substitutions} in the ordering of graph nodes and edges do not {alter-->change} the final output. [ 20 ] Examples include element-wise sum, mean or maximum. It has been {demonstrated-->prove} that GNNs can not be more |expressive| than the Weisfeiler–Lehman Graph Isomorphism Test. [ 21 ] [ 22 ] In practice, this means that there exist different graph structures (e.g., molecules with the same atoms but different bonds) that can not be distinguished by GNNs. More powerful GNNs operating on higher-dimension geometries such as |simplicial| complexes can be designed. [ 23 ] As of 2022, whether or not future architectures will {overcome-->defeat} the message passing primitive is an open research question. [ 8 ]

From this example, a lot of both the strengths and weaknesses of these models can be seen. For example, a lot of the verb forms are not kept, although singular to plural detection and conversion was implemented. Some of the synonyms, such as receptive-->sensory and permutations-->substitutions are somewhat similar, but do not maintain the full level of meaning, as well as the overcome-->defeat section. However, a good substitution was the use of implements-->applies, as this seems to greatly simplify the word for non-english speakers.


This second example shows running the model on the overview section of Machine Learning on Wikipedia:

Machine learning (ML) is a field of {inquiry-->research} devoted to understanding and building methods that '' learn '' – that is, methods that {leverage-->purchase} data to improve performance on some set of tasks. [ 1 ] It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being |explicitly| programmed to do so. [ 2 ] Machine learning algorithms are used in a wide variety of applications, such as in medicine, email {filtering-->filter}, speech recognition, agriculture, and computer vision, where it is difficult or |unfeasible| to develop conventional algorithms to perform the needed tasks. [ 3 ] [ 4 ] A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers, but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on |exploratory| data analysis through |unsupervised| learning. [ 6 ] [ 7 ] Some {implementations-->executions} of machine learning use data and neural networks in a way that |mimics| the working of a biological brain. [ 8 ] [ 9 ] In its application across business problems, machine learning is also referred to as |predictive| |analytics|. Learning algorithms work on the basis that strategies, algorithms, and inferences that worked well in the past are likely to continue working well in the future. These inferences can be |obvious|, such as '' since the sun rose every morning for the last 10,000 days, it will probably rise tomorrow morning as well ''. They can be |nuanced|, such as `` X % of families have geographically separate species with color variants, so there is a Y % chance that |undiscovered| black swans exist ''. [ 10 ] Machine learning programs can perform tasks without being |explicitly| programmed to do so. It involves computers learning from data provided so that they carry out certain tasks. For simple tasks assigned to computers, it is possible to program algorithms telling the machine how to execute all steps required to solve the problem at hand; on the computer's part, no learning is needed. For more advanced tasks, it can be {challenging-->dispute} for a human to |manually| create the needed algorithms. In practice, it can turn out to be more effective to help the machine develop its own algorithm, rather than having human programmers specify every needed step. [ 11 ] The discipline of machine learning {employs-->engages} various approaches to teach computers to {accomplish-->achieve} tasks where no fully {satisfactory-->acceptable} algorithm is available. In cases where vast numbers of potential answers exist, one approach is to label some of the correct answers as valid. This can then be used as training data for the computer to improve the algorithm (s) it uses to determine correct answers. For example, to train a system for the task of digital character recognition, the MNIST |dataset| of |handwritten| digits has often been used.

The first substitution put in here, inquiry-->research, actually works very well in the context of the sentence and significantly reduces its complexity. Some of the other substitutions here may not change complexity very much, for example implementations-->executions. However, it is important to keep in mind that there is bias on how a native english speaker may interpret these substitutions as compared to a non-native one, so it is likely give they frequency that as long as the semantic meaning is correct, a substitution is likely to simplify the text, on average.

This last example shows the application of the model to abstract of the PageRank paper [3]:

The importance of a Web page is an |inherently| sub |jective| matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said |ob| |jectively| about the relative importance of Web pages. This paper describes PageRank, a method for rating Web pages |ob| |jectively| and {mechanically-->automatically}, |effectively| measuring the human interest and attention devoted to them. We compare PageRank to an |idealized| random Web surfer. We show how to eciently {compute-->calculate} PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation. 1 Introduction and Motivation The World Wide Web creates many new challenges for information |retrieval|. It is very large and |heterogeneous|. Current estimates are that there are over 150 million web pages with a {doubling-->double} life of less than one year. More {importantly-->significantly}, the web pages are extremely diverse, ranging from '' What is Joe having for |lunch| today.?  ''to journals about information |retrieval|. In addition to these ma |jor| challenges, search engines on the Web must also {contend-->contest} with |inexperienced| users and pages {engineered-->organize} to {manipulate-->fake} search engine ranking functions. However, unlike `` at '' document collections, the World Wide Web is |hypertext| and provides considerable auxiliary information on top of the text of the web pages, such as link structure and link text. In this paper, we take advantage of the link structure of the Web to produce a global \importance '' ranking of every web page. This ranking, called PageRank, helps search engines and users quickly make sense of the vast |heterogeneity| of the World Wide Web.

In this case, the substitutions of mechanically-->automatically and compute-->calculate did have a significant impact on simplifying the text, as they both get rid of words that may be complicated for no reason. However, the chance of manipulate-->fake does not appear to a good one, as some of the meaning is lost.

# Conclusion

Overall, the use of a combination of Word2Vec and Doc2Vec models appear to show promise, even with the limited English Wikipedia abstracts training set. Some words were successfully simplified, while others had synonyms used that kept the complexity or even had a different meaning in some cases. Given the performance and amount of words for which synonyms were not found, the use of the entire English Wikipedia and a vocabulary set created from the entire Simple English Wikipedia would likely lead to better results when implementing these two models. After running the models on various Wikipedia abstracts in different topics, the results that were achieved are similar to the ones presented, where there are some cases where a simplification is achieved but others where there is slight loss of semantic meaning. Overall however, given the size of the training set, the techniques show promise to be further evaluated on larger datasets for a fully-functioning simplification engine.

# References

The initial papers used upon which the Gensim models were built:

[1] T. Mikolov, K. Chen, G. Corrado, and J. Dean, ‘Efficient Estimation of Word Representations in Vector Space’. arXiv, 2013.

[2] Q. V. Le and T. Mikolov, ‘Distributed Representations of Sentences and Documents’. arXiv, 2014.

[3] L. Page, S. Brin, R. Motwani, and T. Winograd, ‘The PageRank Citation Ranking: Bringing Order to the Web’, Stanford InfoLab, Nov. 1999.


Additional code reference sites can be found in sources.txt.
